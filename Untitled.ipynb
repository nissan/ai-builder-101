{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c8ccdd-fd3f-4bce-b47b-e744d414fd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22bd3efd-2deb-431d-894a-9d29ea1330b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: apache_beam in /home/nissan/mambaforge/lib/python3.10/site-packages (2.49.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (1.7)\n",
      "Requirement already satisfied: orjson<4.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (3.9.5)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (0.3.1.1)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (1.8.2)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (0.18)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (1.57.0)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (2.7.2)\n",
      "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (0.22.0)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (1.24.3)\n",
      "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (0.6.1)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (4.4.1)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (1.22.3)\n",
      "Requirement already satisfied: protobuf<4.24.0,>=3.20.3 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (4.23.4)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (2023.3)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (2023.8.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (4.5.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (0.19.0)\n",
      "Requirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from apache_beam) (11.0.0)\n",
      "Requirement already satisfied: docopt in /home/nissan/mambaforge/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/nissan/mambaforge/lib/python3.10/site-packages (from httplib2<0.23.0,>=0.8->apache_beam) (3.0.9)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from pymongo<5.0.0,>=3.8.0->apache_beam) (2.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install apache_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e3bf4b-e33b-4e65-bab6-81b5aab0b8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/nissan/mambaforge/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (1.57.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (0.33.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/nissan/mambaforge/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/nissan/mambaforge/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913d4312-4d54-40c2-a188-17aad8d4d08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display_html\n",
    "def restartkernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "restartkernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99bb9a02-60d3-496f-bbb0-500c005b15e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-19 16:35:15.989091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-19 16:35:16.996679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "usage: ipykernel_launcher.py [-h] --reddit_table REDDIT_TABLE --output_dir\n",
      "                             OUTPUT_DIR [--dataset_format {TF,JSON}]\n",
      "                             [--parent_depth PARENT_DEPTH]\n",
      "                             [--max_length MAX_LENGTH]\n",
      "                             [--min_length MIN_LENGTH]\n",
      "                             [--train_split TRAIN_SPLIT]\n",
      "                             [--num_shards_test NUM_SHARDS_TEST]\n",
      "                             [--num_shards_train NUM_SHARDS_TRAIN]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --reddit_table, --output_dir\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nissan/mambaforge/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"A Dataflow script for creating datasets from reddit.\n",
    "\n",
    "For usage see README.md.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import argparse\n",
    "import hashlib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from collections import defaultdict, namedtuple\n",
    "from functools import partial\n",
    "\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.io import BigQuerySource, Read\n",
    "from apache_beam.io.textio import WriteToText\n",
    "from apache_beam.io.tfrecordio import WriteToTFRecord\n",
    "from apache_beam.options.pipeline_options import PipelineOptions, SetupOptions\n",
    "\n",
    "_TF_FORMAT = \"TF\"\n",
    "_JSON_FORMAT = \"JSON\"\n",
    "\n",
    "\n",
    "def _parse_args(argv=None):\n",
    "    \"\"\"Parse command line arguments.\"\"\"\n",
    "\n",
    "    def _positive_int(value):\n",
    "        \"\"\"Define a positive integer ArgumentParser type.\"\"\"\n",
    "        value = int(value)\n",
    "        if value <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"Value must be positive, {} was passed.\".format(value))\n",
    "        return value\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--reddit_table\",\n",
    "        required=True,\n",
    "        help=\"The BigQuery table to read comments from, in \"\n",
    "             \"project:table format.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        required=True,\n",
    "        help=\"Google cloud storage output directory to write the dataset.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataset_format\",\n",
    "        choices={_TF_FORMAT, _JSON_FORMAT},\n",
    "        default=\"TF\",\n",
    "        help=\"The dataset format to write. 'TF' for serialized tensorflow \"\n",
    "             \"examples in TFRecords. 'JSON' for text files with one JSON \"\n",
    "             \"object per line.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--parent_depth\",\n",
    "        type=_positive_int,\n",
    "        default=10,\n",
    "        help=\"How many parent comments to consider.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_length\",\n",
    "        type=_positive_int,\n",
    "        default=127,\n",
    "        help=\"Maximum length of comments to include.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_length\",\n",
    "        type=_positive_int,\n",
    "        default=9,\n",
    "        help=\"Minimum length of comments to include.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_split\",\n",
    "        default=0.9, type=float,\n",
    "        help=\"The proportion of data to put in the training set.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_shards_test\",\n",
    "        default=100,\n",
    "        type=_positive_int,\n",
    "        help=\"The number of shards for the test set.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_shards_train\",\n",
    "        default=1000,\n",
    "        type=_positive_int,\n",
    "        help=\"The number of shards for the train set.\",\n",
    "    )\n",
    "    return parser.parse_known_args(argv)\n",
    "\n",
    "\n",
    "# Represent a reddit comment.\n",
    "Comment = namedtuple(\n",
    "    \"Comment\",\n",
    "    [\n",
    "        \"id\",\n",
    "        \"thread_id\",\n",
    "        \"parent_id\",\n",
    "        \"body\",\n",
    "        \"body_is_trimmed\",\n",
    "        \"author\",\n",
    "        \"subreddit\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def normalise_comment(comment, max_length):\n",
    "    \"\"\"Create a _Comment object from a row in the BigQuery table.\"\"\"\n",
    "    return Comment(\n",
    "        id=comment['id'],\n",
    "        thread_id=_normalise_id(comment['link_id']),\n",
    "        parent_id=_normalise_id(comment['parent_id']),\n",
    "        body=trim(comment['body'], max_length),\n",
    "        body_is_trimmed=len(comment['body']) > max_length,\n",
    "        author=comment['author'],\n",
    "        subreddit=comment['subreddit'],\n",
    "    )\n",
    "\n",
    "\n",
    "def _normalise_id(raw_id):\n",
    "    \"\"\"Reddit IDs start with t1_, t2_, etc. which need to be stripped.\"\"\"\n",
    "    return re.sub(\"^t[0-9]_\", \"\", raw_id)\n",
    "\n",
    "\n",
    "def trim(text, max_length):\n",
    "    \"\"\"Trims text to be at most `max_length`, without splitting apart words.\"\"\"\n",
    "    if len(text) <= max_length:\n",
    "        return text\n",
    "\n",
    "    text = text[:max_length + 1]\n",
    "\n",
    "    # Trim until the last two characters are the boundary between an\n",
    "    # alphanumeric character, and a non-alphanumeric character.\n",
    "    while len(text) > 1 and (text[-1].isalnum() == text[-2].isalnum()):\n",
    "        text = text[:-1]\n",
    "\n",
    "    return text[:-1]\n",
    "\n",
    "\n",
    "def _should_skip(comment, min_length):\n",
    "    if comment.body_is_trimmed:\n",
    "        return True\n",
    "    if comment.body in {\"[deleted]\", \"[removed]\"}:\n",
    "        return True\n",
    "    if len(comment.body) < min_length:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def create_examples(thread, parent_depth, min_length, format):\n",
    "    \"\"\"Creates serialized tensorflow examples from a reddit thread.\"\"\"\n",
    "    id_to_comment = {comment.id: comment for comment in list(thread)}\n",
    "\n",
    "    for linear_path in linear_paths(id_to_comment, parent_depth):\n",
    "        response = id_to_comment[linear_path[-1]]\n",
    "        context = id_to_comment[linear_path[-2]]  # guaranteed to exist.\n",
    "\n",
    "        if (_should_skip(response, min_length)\n",
    "                or _should_skip(context, min_length)):\n",
    "            continue\n",
    "\n",
    "        example = {}\n",
    "        example['subreddit'] = response.subreddit\n",
    "        example['thread_id'] = response.thread_id\n",
    "        example['context_author'] = context.author\n",
    "        example['response_author'] = response.author\n",
    "        example['context'] = context.body\n",
    "        example['response'] = response.body\n",
    "\n",
    "        for i in range(parent_depth - 1):\n",
    "            # Extra contexts start at index -3.\n",
    "            index = -3 - i\n",
    "            try:\n",
    "                context_i = linear_path[index]\n",
    "            except IndexError:\n",
    "                break\n",
    "\n",
    "            example['context/{}'.format(i)] = id_to_comment[context_i].body\n",
    "\n",
    "        yield example\n",
    "\n",
    "\n",
    "def _features_to_serialized_tf_example(features):\n",
    "    \"\"\"Convert a string dict to a serialized TF example.\n",
    "\n",
    "    The dictionary maps feature names (strings) to feature values (strings).\n",
    "    \"\"\"\n",
    "    example = tf.train.Example()\n",
    "    for feature_name, feature_value in features.items():\n",
    "        example.features.feature[feature_name].bytes_list.value.append(\n",
    "            feature_value.encode(\"utf-8\"))\n",
    "    return example.SerializeToString()\n",
    "\n",
    "\n",
    "def linear_paths(id_to_comment, parent_depth):\n",
    "    \"\"\"Gets all linear paths of comments and replies from the thread.\n",
    "\n",
    "    Each linear path is guaranteed to have at least two comments in it.\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    seen_ids = set()\n",
    "    id_to_children = defaultdict(list)\n",
    "    for comment_id, comment in id_to_comment.items():\n",
    "        id_to_children[comment.parent_id].append(comment_id)\n",
    "        if comment.parent_id not in id_to_comment:\n",
    "            paths.append([comment_id])\n",
    "            seen_ids.add(comment_id)\n",
    "\n",
    "    while paths:\n",
    "        new_paths = []\n",
    "        for path in paths:\n",
    "            last_id = path[-1]\n",
    "            for child_id in id_to_children[last_id]:\n",
    "                if child_id in seen_ids:\n",
    "                    # Prevent infinite loops.\n",
    "                    continue\n",
    "                seen_ids.add(child_id)\n",
    "                new_path = path[-parent_depth:] + [child_id]\n",
    "                new_paths.append(new_path)\n",
    "                yield new_path\n",
    "        paths = new_paths\n",
    "\n",
    "\n",
    "def _shuffle(pcollection):\n",
    "    \"\"\"Shuffles the input pcollection.\"\"\"\n",
    "    pcollection |= \"add random key\" >> beam.Map(\n",
    "        lambda value: (uuid.uuid4(), value))\n",
    "    pcollection |= \"group by key\" >> beam.GroupByKey()\n",
    "    pcollection |= \"get shuffled values\" >> beam.FlatMap(lambda t: t[1])\n",
    "    return pcollection\n",
    "\n",
    "\n",
    "class _TrainTestSplitFn(beam.DoFn):\n",
    "    \"\"\"Splits an input PCollection of examples into train and test.\n",
    "\n",
    "    This uses the thread id to compute the split, so that examples from the\n",
    "    same thread are in the same set. The split is deterministic based on\n",
    "    thread id, so that multiple runs produce the same result.\n",
    "    \"\"\"\n",
    "\n",
    "    TRAIN_TAG = \"train\"\n",
    "    TEST_TAG = \"test\"\n",
    "\n",
    "    def __init__(self, train_split, num_buckets=4096):\n",
    "        super(_TrainTestSplitFn, self).__init__()\n",
    "        self._train_split = train_split\n",
    "        self._num_buckets = num_buckets\n",
    "\n",
    "    def process(self, example):\n",
    "        split_value = self._split_value(example['thread_id'])\n",
    "        split = (\n",
    "            self.TRAIN_TAG if split_value < self._train_split else\n",
    "            self.TEST_TAG)\n",
    "        yield pvalue.TaggedOutput(split, example)\n",
    "\n",
    "    def _split_value(self, thread_id):\n",
    "        \"\"\"Compute a value from 0 to 1 used to compute the split.\"\"\"\n",
    "        md5 = hashlib.md5()\n",
    "        md5.update(thread_id)\n",
    "        md5_digest = int(md5.hexdigest(), 16)\n",
    "        return (\n",
    "            (1 + md5_digest % self._num_buckets)\n",
    "            / float(self._num_buckets)\n",
    "        )\n",
    "\n",
    "\n",
    "def run(argv=None, comments=None):\n",
    "    \"\"\"Run the beam pipeline.\n",
    "\n",
    "    Args:\n",
    "        argv: (optional) the command line flags to parse.\n",
    "        comments_collection: (optional) a list of comment JSON objects to\n",
    "            process. Used in unit-tests to avoid requiring a BigQuery source.\n",
    "    \"\"\"\n",
    "    args, pipeline_args = _parse_args(argv)\n",
    "\n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    p = beam.Pipeline(options=pipeline_options)\n",
    "\n",
    "    if comments is not None:\n",
    "        comments = p | (\"Read in-memory comments\") >> beam.Create(comments)\n",
    "    else:\n",
    "        comments = p | (\"Read \" + args.reddit_table) >> Read(\n",
    "            BigQuerySource(args.reddit_table))\n",
    "\n",
    "    comments |= (\n",
    "        \"Normalise comments\" >> beam.Map(\n",
    "            partial(normalise_comment, max_length=args.max_length)))\n",
    "\n",
    "    thread_id_to_comments = comments | (\n",
    "        \"Key by thread id\" >> beam.Map(\n",
    "            lambda comment: (comment.thread_id, comment)))\n",
    "    threads = thread_id_to_comments | (\n",
    "        \"Group comments by thread ID\" >> beam.GroupByKey())\n",
    "    threads = threads | (\"Get threads\" >> beam.Map(lambda t: t[1]))\n",
    "\n",
    "    examples = threads | (\n",
    "        \"Create {} examples\".format(args.dataset_format) >> beam.FlatMap(\n",
    "            partial(create_examples,\n",
    "                    parent_depth=args.parent_depth,\n",
    "                    min_length=args.min_length,\n",
    "                    format=args.dataset_format,\n",
    "                    )))\n",
    "    examples = _shuffle(examples)\n",
    "\n",
    "    examples |= \"split train and test\" >> beam.ParDo(\n",
    "        _TrainTestSplitFn(train_split=args.train_split)\n",
    "    ).with_outputs(_TrainTestSplitFn.TEST_TAG, _TrainTestSplitFn.TRAIN_TAG)\n",
    "\n",
    "    if args.dataset_format == _JSON_FORMAT:\n",
    "        write_sink = WriteToText\n",
    "        file_name_suffix = \".json\"\n",
    "        serialize_fn = json.dumps\n",
    "    else:\n",
    "        assert args.dataset_format == _TF_FORMAT\n",
    "        write_sink = WriteToTFRecord\n",
    "        file_name_suffix = \".tfrecord\"\n",
    "        serialize_fn = _features_to_serialized_tf_example\n",
    "\n",
    "    for name, tag in [(\"train\", _TrainTestSplitFn.TRAIN_TAG),\n",
    "                      (\"test\", _TrainTestSplitFn.TEST_TAG)]:\n",
    "\n",
    "        serialized_examples = examples[tag] | (\n",
    "            \"serialize {} examples\".format(name) >> beam.Map(serialize_fn))\n",
    "        (\n",
    "            serialized_examples | (\"write \" + name)\n",
    "            >> write_sink(\n",
    "                os.path.join(args.output_dir, name),\n",
    "                file_name_suffix=file_name_suffix,\n",
    "                num_shards=args.num_shards_train,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    result = p.run()\n",
    "    result.wait_until_finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a93871-4ba2-49aa-95ab-0ac29530d83c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
