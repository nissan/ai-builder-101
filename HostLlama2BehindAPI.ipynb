{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bznyRLfLg750"
   },
   "source": [
    "# **Hosting Llama 2 with Free GPU via Google Collab**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRDd_-CFIqRV"
   },
   "source": [
    "**Before getting started, if running on Google Colab, check that the runtime is set to T4 GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kU0GYiR4hWKJ"
   },
   "source": [
    "## Install Dependencies\n",
    "- Requirements for running FastAPI Server\n",
    "- Requirements for creating a public model serving URL via Ngrok\n",
    "- Requirements for running Llama2 13B (including Quantization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gDVaaatLEpzq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.1.81.tar.gz (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from llama-cpp-python) (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from llama-cpp-python) (1.24.3)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.81-cp310-cp310-linux_x86_64.whl size=971718 sha256=2399bbd077ecdfd1b579908d69fb4ddfd7c1805909a442668dd2eadc816e4366\n",
      "  Stored in directory: /home/nissan/.cache/pip/wheels/99/27/c0/ccda486d8dc16ebb8c61d9d7959380eced2cb3279cf0145dfe\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.1 llama-cpp-python-0.1.81\n"
     ]
    }
   ],
   "source": [
    "# Build Llama cpp\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "u24fGvOmT2Pi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi[all] in /home/nissan/mambaforge/lib/python3.10/site-packages (0.99.1)\n",
      "Requirement already satisfied: uvicorn in /home/nissan/mambaforge/lib/python3.10/site-packages (0.23.2)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers in /home/nissan/mambaforge/lib/python3.10/site-packages (4.32.0)\n",
      "Requirement already satisfied: pydantic in /home/nissan/mambaforge/lib/python3.10/site-packages (1.10.12)\n",
      "Requirement already satisfied: tensorflow in /home/nissan/mambaforge/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from fastapi[all]) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from fastapi[all]) (4.5.0)\n",
      "Collecting email-validator>=1.1.1 (from fastapi[all])\n",
      "  Downloading email_validator-2.0.0.post2-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from fastapi[all]) (0.24.1)\n",
      "Collecting itsdangerous>=1.1.0 (from fastapi[all])\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.2 in /home/nissan/mambaforge/lib/python3.10/site-packages (from fastapi[all]) (3.1.2)\n",
      "Requirement already satisfied: orjson>=3.2.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from fastapi[all]) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from fastapi[all]) (6.0)\n",
      "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi[all])\n",
      "  Obtaining dependency information for ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 from https://files.pythonhosted.org/packages/78/90/bfa62616208bd5195a113c0aa4e42c9f471e69edfc48feba6a0ab494cccb/ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: click>=7.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from uvicorn) (8.1.6)\n",
      "Requirement already satisfied: h11>=0.8 in /home/nissan/mambaforge/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: filelock in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (1.57.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorflow) (0.33.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from email-validator>=1.1.1->fastapi[all]) (2.4.2)\n",
      "Requirement already satisfied: idna>=2.0.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from email-validator>=1.1.1->fastapi[all]) (3.4)\n",
      "Requirement already satisfied: certifi in /home/nissan/mambaforge/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi[all]) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi[all]) (0.17.3)\n",
      "Requirement already satisfied: sniffio in /home/nissan/mambaforge/lib/python3.10/site-packages (from httpx>=0.23.0->fastapi[all]) (1.3.0)\n",
      "Requirement already satisfied: fsspec in /home/nissan/mambaforge/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from jinja2>=2.11.2->fastapi[all]) (2.1.3)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi[all]) (3.7.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from uvicorn) (0.6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/nissan/mambaforge/lib/python3.10/site-packages (from uvicorn) (1.0.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from uvicorn) (0.17.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/nissan/mambaforge/lib/python3.10/site-packages (from uvicorn) (0.19.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/nissan/mambaforge/lib/python3.10/site-packages (from uvicorn) (11.0.3)\n",
      "Requirement already satisfied: exceptiongroup in /home/nissan/mambaforge/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi[all]) (1.1.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/nissan/mambaforge/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/nissan/mambaforge/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ujson, python-multipart, itsdangerous, email-validator\n",
      "Successfully installed email-validator-2.0.0.post2 itsdangerous-2.1.2 python-multipart-0.0.6 ujson-5.8.0\n"
     ]
    }
   ],
   "source": [
    "# If this complains about dependency resolver, it's safe to ignore\n",
    "!pip install fastapi[all] uvicorn python-multipart transformers pydantic tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Kxx3xpr2cXHS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-28 21:41:13--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
      "Resolving bin.equinox.io (bin.equinox.io)... 18.205.222.128, 52.202.168.65, 54.161.241.46, ...\n",
      "Connecting to bin.equinox.io (bin.equinox.io)|18.205.222.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13921656 (13M) [application/octet-stream]\n",
      "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
      "\n",
      "ngrok-stable-linux- 100%[===================>]  13.28M  2.19MB/s    in 6.9s    \n",
      "\n",
      "2023-08-28 21:41:21 (1.93 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
      "\n",
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "  inflating: ngrok                   \n"
     ]
    }
   ],
   "source": [
    "# This downloads and sets up the Ngrok executable in the Google Colab instance\n",
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip -o ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPOYhvtAHiW3"
   },
   "source": [
    "Ngrok is used to make the FastAPI server accessible via a public URL.\n",
    "\n",
    "Users are required to make a free account and provide their auth token to use Ngrok. The free version only allows 1 local tunnel and the auth token is used to track this usage limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nw3LWCfNcg9E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /home/nissan/.ngrok2/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "# https://dashboard.ngrok.com/signup\n",
    "!./ngrok authtoken 2UbsyiZi4ktGCts7yq0Hpql0agQ_4oMqiX8dQoxmSALmtyUTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUqG0_-6IChq"
   },
   "source": [
    "## Create FastAPI App\n",
    "This provides an API to the Llama 2 model. The model version can be changed in the code below as desired.\n",
    "\n",
    "For this demo we will use the 13 billion parameter version which is finetuned for instruction (chat) following.\n",
    "\n",
    "Despite the compression, it is still a more powerful model than the 7B variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Z6F078OHVssf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "from typing import Any\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from fastapi import HTTPException\n",
    "from pydantic import BaseModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# GGML model required to fit Llama2-13B on a T4 GPU\n",
    "GENERATIVE_AI_MODEL_REPO = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
    "GENERATIVE_AI_MODEL_FILE = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=GENERATIVE_AI_MODEL_REPO,\n",
    "    filename=GENERATIVE_AI_MODEL_FILE\n",
    ")\n",
    "\n",
    "llama2_model = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=64,\n",
    "    n_ctx=2000\n",
    ")\n",
    "\n",
    "# Test an inference\n",
    "print(llama2_model(prompt=\"Hello \", max_tokens=1))\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "# This defines the data json format expected for the endpoint, change as needed\n",
    "class TextInput(BaseModel):\n",
    "    inputs: str\n",
    "    parameters: dict[str, Any] | None\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def status_gpu_check() -> dict[str, str]:\n",
    "    gpu_msg = \"Available\" if tf.test.is_gpu_available() else \"Unavailable\"\n",
    "    return {\n",
    "        \"status\": \"I am ALIVE!\",\n",
    "        \"gpu\": gpu_msg\n",
    "    }\n",
    "\n",
    "\n",
    "@app.post(\"/generate/\")\n",
    "async def generate_text(data: TextInput) -> dict[str, str]:\n",
    "    try:\n",
    "        params = data.parameters or {}\n",
    "        response = llama2_model(prompt=data.inputs, **params)\n",
    "        model_out = response['choices'][0]['text']\n",
    "        return {\"generated_text\": model_out}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PikyQUQKIewj"
   },
   "source": [
    "## Start FastAPI Server\n",
    "The initial run will take a long time due to having to download the model and load it onto GPU.\n",
    "\n",
    "Note: interrupting the Google Colab runtime will send a SIGINT and stop the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HErEYVtPcGg9"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Background processes not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This cell finishes quickly because it just needs to start up the server\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# The server will start the model download and will take a while to start up\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# ~5 minutes\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muvicorn app:app --host 0.0.0.0 --port 8000 > server.log 2>&1 &\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.10/site-packages/ipykernel/zmqshell.py:639\u001b[0m, in \u001b[0;36mZMQInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cmd\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;66;03m# this is *far* from a rigorous test\u001b[39;00m\n\u001b[1;32m    634\u001b[0m     \u001b[38;5;66;03m# We do not support backgrounding processes because we either use\u001b[39;00m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# pexpect or pipes to read from.  Users can always just call\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# os.system() or use ip.system=ip.system_raw\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;66;03m# if they really want a background process.\u001b[39;00m\n\u001b[1;32m    638\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    641\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;66;03m# Also, protect system call from UNC paths on Windows here too\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# as is done in InteractiveShell.system_raw\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: Background processes not supported."
     ]
    }
   ],
   "source": [
    "# This cell finishes quickly because it just needs to start up the server\n",
    "# The server will start the model download and will take a while to start up\n",
    "# ~5 minutes\n",
    "!uvicorn app:app --host 0.0.0.0 --port 8000 > server.log 2>&1 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tunHZmTHX98z"
   },
   "source": [
    "Check the logs at server.log to see progress.\n",
    "\n",
    "Wait until model is loaded and check with the next cell before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJUJ4vICfQ7a"
   },
   "outputs": [],
   "source": [
    "# If you see \"Failed to connect\", it's because the server is still starting up\n",
    "# Wait for the model to be downloaded and the server to fully start\n",
    "# Check the server.log file to see the status\n",
    "!curl localhost:8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csO2FA7LKWXw"
   },
   "source": [
    "## Use Ngrok to create a public URL for the FastAPI server.\n",
    "**IMPORTANT:** If you created an account via email, please verify your email or the next 2 cells won't work.\n",
    "\n",
    "If you signed up via Google or GitHub account, you're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c-FpVEkifNdS"
   },
   "outputs": [],
   "source": [
    "# This starts Ngrok and creates the public URL\n",
    "from IPython import get_ipython\n",
    "get_ipython().system_raw('./ngrok http 8000 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XxzFtg3_Kfot"
   },
   "source": [
    "Check the URL generated by the next cell, it should report that the FastAPI server is alive and that GPU is available.\n",
    "\n",
    "To hit the model endpoint, simply add `/generate` to the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCAIkVuxc3JU"
   },
   "outputs": [],
   "source": [
    "# Get the Public URL\n",
    "# If this doesn't work, make sure you verified your email\n",
    "# Then run the previous code cell and this one again\n",
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liqVEsGfZPse"
   },
   "source": [
    "## Shutting Down\n",
    "To shut down the processes, run the following commands in a new cell:\n",
    "```\n",
    "!pkill uvicorn\n",
    "!pkill ngrok\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
