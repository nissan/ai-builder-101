{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXDVdo_AVSii"
   },
   "source": [
    "# Reinforcement Learning from AI Feedback\n",
    "\n",
    "Fine-tunes a language model using natural language criteria for its sampled outputs.\n",
    "\n",
    "This notebook fine-tunes [EleutherAI](https://www.eleuther.ai/)'s [Pythia 160M](https://huggingface.co/EleutherAI/pythia-160m-deduped) language model using a zero-shot reward model derived from an instruct tuned language model ([Katherine Crowson's instruct fine-tune](https://huggingface.co/RiversHaveWings/minihf_evaluator_openllama_7b) of [OpenLLaMA 7B](https://huggingface.co/openlm-research/open_llama_7b)).\n",
    "\n",
    "The zero-shot reward model is obtained by asking the instruct model yes/no questions about the generations from the model that is being RLAIF tuned. It takes the logits for the first token of the response and forms a binary classifier logit as `log(p(yes) + p(neither) / 2) - log(p(no) + p(neither) / 2)`. It uses `log(sigmoid(logit))` (log probability of the \"yes\" class) as the reward. It uses weighted \"soft conjunctions\" of multiple binary classifier logits to fine-tune the model to satisfy multiple natural language criteria simultaneously.\n",
    "\n",
    "The gradient estimator is [DiCE](https://github.com/crowsonkb/dice-mc), a variant of REINFORCE. It uses a fixed strength KL penalty to constrain the fine-tuned model's distribution over tokens to not vary too far from the original model's.\n",
    "\n",
    "If you like this notebook you should check out [MiniHF](https://github.com/JD-P/minihf/), the language model fine-tuning and inference tool the code was originally written for.\n",
    "\n",
    "<small>Notebook by Katherine Crowson (crowsonkb@gmail.com, https://twitter.com/RiversHaveWings)\n",
    "<br>Sponsored by StabilityAI (https://twitter.com/stabilityai)\n",
    "<br>Copyright 2023 Katherine Crowson. Licensed under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0).</small>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V09dXM8RVLyY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 28 15:55:32 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.98.01              Driver Version: 536.99       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1660 Ti     On  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   61C    P8               7W /  80W |    200MiB /  6144MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA TITAN RTX               On  | 00000000:14:00.0 Off |                  N/A |\n",
      "| 41%   28C    P8               2W / 280W |  16734MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        20      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        20      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        21      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        23      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A        23      G   /Xwayland                                 N/A      |\n",
      "|    0   N/A  N/A     13663      C   /python3.10                               N/A      |\n",
      "|    1   N/A  N/A        20      G   /Xwayland                                 N/A      |\n",
      "|    1   N/A  N/A        20      G   /Xwayland                                 N/A      |\n",
      "|    1   N/A  N/A        21      G   /Xwayland                                 N/A      |\n",
      "|    1   N/A  N/A        23      G   /Xwayland                                 N/A      |\n",
      "|    1   N/A  N/A        23      G   /Xwayland                                 N/A      |\n",
      "|    1   N/A  N/A     13663      C   /python3.10                               N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#@title Check GPU\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA TITAN RTX'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6IQqn7CaWmBl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /home/nissan/mambaforge/lib/python3.10/site-packages (0.41.1)\n",
      "Requirement already satisfied: dice-mc in /home/nissan/mambaforge/lib/python3.10/site-packages (0.1.1)\n",
      "Requirement already satisfied: peft in /home/nissan/mambaforge/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: safetensors in /home/nissan/mambaforge/lib/python3.10/site-packages (0.3.2)\n",
      "Requirement already satisfied: sentencepiece in /home/nissan/mambaforge/lib/python3.10/site-packages (0.1.99)\n",
      "Requirement already satisfied: tokenizers in /home/nissan/mambaforge/lib/python3.10/site-packages (0.13.3)\n",
      "Requirement already satisfied: transformers in /home/nissan/mambaforge/lib/python3.10/site-packages (4.32.0)\n",
      "Requirement already satisfied: torch in /home/nissan/mambaforge/lib/python3.10/site-packages (from dice-mc) (2.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nissan/mambaforge/lib/python3.10/site-packages (from peft) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from peft) (23.1)\n",
      "Requirement already satisfied: psutil in /home/nissan/mambaforge/lib/python3.10/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/nissan/mambaforge/lib/python3.10/site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: tqdm in /home/nissan/mambaforge/lib/python3.10/site-packages (from peft) (4.65.0)\n",
      "Requirement already satisfied: accelerate in /home/nissan/mambaforge/lib/python3.10/site-packages (from peft) (0.22.0)\n",
      "Requirement already satisfied: filelock in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/nissan/mambaforge/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: fsspec in /home/nissan/mambaforge/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/nissan/mambaforge/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: sympy in /home/nissan/mambaforge/lib/python3.10/site-packages (from torch->dice-mc) (1.12)\n",
      "Requirement already satisfied: networkx in /home/nissan/mambaforge/lib/python3.10/site-packages (from torch->dice-mc) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/nissan/mambaforge/lib/python3.10/site-packages (from torch->dice-mc) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nissan/mambaforge/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nissan/mambaforge/lib/python3.10/site-packages (from jinja2->torch->dice-mc) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/nissan/mambaforge/lib/python3.10/site-packages (from sympy->torch->dice-mc) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#@title Install dependencies\n",
    "\n",
    "!pip install bitsandbytes dice-mc peft safetensors sentencepiece tokenizers transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EWcVLujbbDre"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 16:01:50.445428: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 16:01:51.275483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#@title Import libraries\n",
    "\n",
    "from functools import partial\n",
    "import math\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "\n",
    "import dice_mc.torch as dice\n",
    "import peft\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "DXfUnfhUbj05"
   },
   "outputs": [],
   "source": [
    "#@title Define functions\n",
    "\n",
    "print = tqdm.external_write_mode()(print)\n",
    "\n",
    "\n",
    "def endless_range(start=0, step=1):\n",
    "    i = start\n",
    "    while True:\n",
    "        yield i\n",
    "        i += step\n",
    "\n",
    "\n",
    "def at_least_float32(tensor):\n",
    "    dtype = torch.promote_types(tensor.dtype, torch.float32)\n",
    "    return tensor.to(dtype)\n",
    "\n",
    "\n",
    "def logsumexp_scaled(a, b, return_sign=False, dim=None, keepdim=False):\n",
    "    \"\"\"Compute log(sum(b * exp(a))).\"\"\"\n",
    "    if dim is None:\n",
    "        dim = tuple(range(a.ndim))\n",
    "\n",
    "    a, b = torch.broadcast_tensors(a, b)\n",
    "    a = torch.where(b != 0, a, float(\"-inf\"))\n",
    "\n",
    "    a_max = torch.amax(a, dim=dim, keepdim=True)\n",
    "    a_max = torch.nan_to_num(a_max, 0.0, 0.0, 0.0)\n",
    "\n",
    "    tmp = b * torch.exp(a - a_max)\n",
    "\n",
    "    s = torch.sum(tmp, dim=dim, keepdim=keepdim)\n",
    "    if return_sign:\n",
    "        sgn = torch.sign(s)\n",
    "        s *= sgn\n",
    "    out = torch.log(s)\n",
    "\n",
    "    if not keepdim:\n",
    "        a_max = torch.squeeze(a_max, dim=dim)\n",
    "    out += a_max\n",
    "\n",
    "    if return_sign:\n",
    "        return out, sgn\n",
    "    else:\n",
    "        return out\n",
    "\n",
    "\n",
    "def soft_maximum(values, weights=None, tau=1.0, dim=None, keepdim=False):\n",
    "    if weights is None:\n",
    "        weights = torch.ones_like(values)\n",
    "    weights /= weights.sum(dim=dim, keepdim=True)\n",
    "    return logsumexp_scaled(values / tau, weights, dim=dim, keepdim=keepdim) * tau\n",
    "\n",
    "\n",
    "def soft_minimum(values, weights=None, tau=1.0, dim=None, keepdim=False):\n",
    "    if weights is None:\n",
    "        weights = torch.ones_like(values)\n",
    "    weights /= weights.sum(dim=dim, keepdim=True)\n",
    "    return -logsumexp_scaled(-values / tau, weights, dim=dim, keepdim=keepdim) * tau\n",
    "\n",
    "\n",
    "def get_scores_from_logits(logits, pos_tokens, neg_tokens):\n",
    "    logits = at_least_float32(logits[:, -1, :])\n",
    "    logits = F.log_softmax(logits, dim=-1)\n",
    "    pos = torch.logsumexp(logits[:, pos_tokens], dim=-1)\n",
    "    neg = torch.logsumexp(logits[:, neg_tokens], dim=-1)\n",
    "    rest = (1 - pos.exp() - neg.exp()).log()\n",
    "    return torch.logaddexp(pos, rest - math.log(2)) - torch.logaddexp(neg, rest - math.log(2))\n",
    "\n",
    "\n",
    "def find_token_for_string(tokenizer, prefix, s):\n",
    "    tok_prefix = tokenizer(prefix).input_ids\n",
    "    tok_prefix_s = tokenizer(prefix + s).input_ids\n",
    "    if tok_prefix_s[: len(tok_prefix)] != tok_prefix:\n",
    "        raise RuntimeError(f\"{prefix!r} tokens are not a prefix of {prefix + s!r} tokens\")\n",
    "    return tok_prefix_s[len(tok_prefix)]\n",
    "\n",
    "\n",
    "def find_tokens_for_strings(tokenizer, prefix, strings):\n",
    "    return sorted(set([find_token_for_string(tokenizer, prefix, s) for s in strings]))\n",
    "\n",
    "\n",
    "def make_get_scores(tokenizer, prefix):\n",
    "    pos_tokens = find_tokens_for_strings(tokenizer, prefix, [\"yes\", \"Yes\", \"YES\"])\n",
    "    neg_tokens = find_tokens_for_strings(tokenizer, prefix, [\"no\", \"No\", \"NO\"])\n",
    "    return partial(get_scores_from_logits, pos_tokens=pos_tokens, neg_tokens=neg_tokens)\n",
    "\n",
    "\n",
    "def kl_div_est(logp, logq):\n",
    "    \"\"\"Biased estimator of D_KL(P || Q) from log(p(x)) and log(q(x)), x sampled from p.\"\"\"\n",
    "    return torch.logaddexp(logp - logq, logq - logp) - math.log(2)\n",
    "\n",
    "\n",
    "def inv_cumsum(x):\n",
    "    \"\"\"Inverse of cumulative sum.\"\"\"\n",
    "    out = x.clone()\n",
    "    out[..., 1:] -= x[..., :-1]\n",
    "    return out\n",
    "\n",
    "\n",
    "def gradient_norm(params):\n",
    "    params = list(params)\n",
    "    total = params[0].new_tensor(0.0)\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            total += p.grad.pow(2).sum()\n",
    "    return total.sqrt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "I7NCwCJ6cI3D"
   },
   "outputs": [],
   "source": [
    "#@title Define evaluator templates and prompts\n",
    "\n",
    "templates = [\n",
    "    \"\"\"Answer yes or no and only yes or no.\n",
    "\n",
    "=== Begin story ===\n",
    "{text}\n",
    "=== End story ===\n",
    "\n",
    "Does this story make the reader feel like crying?\"\"\",\n",
    "    \"\"\"Answer yes or no and only yes or no.\n",
    "\n",
    "=== Begin story ===\n",
    "{text}\n",
    "=== End story ===\n",
    "\n",
    "Is this story well-written and coherent?\"\"\",\n",
    "]\n",
    "weights = [1.0, 0.5]\n",
    "signs = [1, 1]\n",
    "\n",
    "\n",
    "def make_evaluator_prompts(texts):\n",
    "    return [[template.format(text=text) + \"<|end|>\" for text in texts] for template in templates]\n",
    "\n",
    "\n",
    "train_prompts = [\n",
    "    \"My cat is so cute, but\",\n",
    "    \"I was watching TV, and\",\n",
    "    \"She looked in the mirror and\",\n",
    "    \"Alice said, \\\"\",\n",
    "]\n",
    "\n",
    "eval_prompts = train_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "id": "hNbkWo6JfEMN"
   },
   "outputs": [],
   "source": [
    "#@title Training parameters\n",
    "\n",
    "#@markdown Batch size:\n",
    "bs = 12  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown Number of tokens to sample per batch item:\n",
    "n_tokens = 48  #@param {type:\"integer\"}\n",
    "\n",
    "#@markdown KL penalty weight:\n",
    "#@markdown <br><small>Constrains the fine-tuned model to be close to the original model. The larger the KL penalty, the less it is allowed to deviate from the original model's distribution.</small>\n",
    "kl_weight = 1.0  #@param {type:\"number\"}\n",
    "\n",
    "#@markdown Temperature for soft conjunction:\n",
    "#@markdown <br><small>Interpolates between the weighted mean of the reward components (evaluator templates) and their minimum. Higher temperature is more mean-like, lower is more minimum-like.</small>\n",
    "tau = 1.0  #@param {type:\"number\"}\n",
    "\n",
    "#@markdown Save every this many steps:\n",
    "save_every = 250  #@param {type:\"integer\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ceJddybicwXg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluator model tokenizer...\n",
      "Loading evaluator base model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440c12af744b415aa62721b141715760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluator adapter...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#@title Load evaluator model\n",
    "\n",
    "# Use small-shard safetensors version of openlm-research/open_llama_7b to be\n",
    "# able to load the model on non-high RAM Colab instances\n",
    "eval_model_name = \"RiversHaveWings/open_llama_7b_safetensors\"\n",
    "eval_adapter_name = \"RiversHaveWings/minihf_evaluator_openllama_7b\"\n",
    "\n",
    "print(\"Loading evaluator model tokenizer...\")\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(eval_adapter_name)\n",
    "eval_tokenizer.padding_side = \"left\"\n",
    "\n",
    "print(\"Loading evaluator base model...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "eval_model = AutoModelForCausalLM.from_pretrained(\n",
    "    eval_model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\"Loading evaluator adapter...\")\n",
    "eval_model = peft.PeftModel.from_pretrained(eval_model, eval_adapter_name)\n",
    "eval_model.requires_grad_(False);\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0NFQoa1veHMx"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635780c4d0cc4d30995472107e166df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "My cat is so cute, but perfect for you!” Vlad will come home to find him drawing\n",
      "down a sleeve.  Sixth grade”University-Madison”L on Brian’s cup as a black\n",
      "instructor.  6th grade“  Tuning\n",
      "===\n",
      "I was watching TV, and even though a water-closet and timeslot IDI was heading\n",
      "its way (I LOVE my non-alcoholic beverage range) here on my weekend at Mumm and\n",
      "TPI so I was pretty sure if I travlnse\n",
      "===\n",
      "She looked in the mirror and saw it was the kid at the County Suites in Sprold.\n",
      "They were so happy to see him walking. Linda gave a halfhearted hug for it,\n",
      "knowing... somehow, 10 min. to take care of his baby would jeopard\n",
      "===\n",
      "Alice said, \"I was Wrong.\" \"Damn it.\" \"Heard that one about driving yesterday?\"\n",
      "\"Yeah.\" \"He's gonna get it now.\" \"Morning, noble.\" \"So your piece is up.\"\n",
      "\"Mindful of what you're\n",
      "======\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 99\u001b[0m\n\u001b[1;32m     96\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Print metrics\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124mg\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, main: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_main\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124mg\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, kl: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_kl\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124mg\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, grad norm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrad_norm\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124mg\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Take an optimizer step\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 105\u001b[0m, in \u001b[0;36mgradient_norm\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total\u001b[38;5;241m.\u001b[39msqrt()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1!"
     ]
    }
   ],
   "source": [
    "#@title Training loop\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "output_path = \"model\"\n",
    "\n",
    "train_inputs = tokenizer(train_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "eval_inputs = tokenizer(eval_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "input_n, input_len = train_inputs.input_ids.shape\n",
    "get_scores = make_get_scores(eval_tokenizer, \"<|end|>\")\n",
    "weights_ = torch.tensor(weights, device=device)[None]\n",
    "signs_ = torch.tensor(signs, device=device)[None]\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98))\n",
    "baseline = dice.EMABaseline(decay=0.98).to(device)\n",
    "baseline_kl = dice.EMABaseline(decay=0.98).to(device)\n",
    "\n",
    "\n",
    "for i in tqdm(endless_range()):\n",
    "    # Demo generations\n",
    "    if i % 50 == 0:\n",
    "        outputs = model.generate(\n",
    "            eval_inputs.input_ids,\n",
    "            attention_mask=eval_inputs.attention_mask,\n",
    "            do_sample=True,\n",
    "            min_new_tokens=n_tokens,\n",
    "            max_new_tokens=n_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            top_k=0,\n",
    "        )\n",
    "        texts = [tokenizer.decode(toks, skip_special_tokens=True) for toks in outputs]\n",
    "        print(\"======\")\n",
    "        print(\"\\n===\\n\".join(textwrap.fill(text, width=80) for text in texts))\n",
    "        print(\"======\")\n",
    "\n",
    "    # Save model\n",
    "    if i > 0 and i % save_every == 0:\n",
    "        print(\"Saving model...\")\n",
    "        tokenizer.save_pretrained(output_path)\n",
    "        model.save_pretrained(output_path, safe_serialization=True)\n",
    "\n",
    "    # Sample from training prompts\n",
    "    indices = torch.randint(0, input_n, [bs], device=device)\n",
    "    tokens = model.generate(\n",
    "        train_inputs.input_ids[indices],\n",
    "        attention_mask=train_inputs.attention_mask[indices],\n",
    "        do_sample=True,\n",
    "        min_new_tokens=n_tokens,\n",
    "        max_new_tokens=n_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        top_k=0,\n",
    "    )\n",
    "\n",
    "    # Get logits with grad for backprop\n",
    "    attention_mask = torch.cat(\n",
    "        [train_inputs.attention_mask[indices], torch.ones_like(tokens[:, input_len:])], dim=1\n",
    "    )\n",
    "    outputs = model(tokens, attention_mask=attention_mask)\n",
    "\n",
    "    # Create stochastic nodes\n",
    "    logp = dice.logp_categorical(outputs.logits[:, input_len - 1 : -1], tokens[:, input_len:])\n",
    "    logp_sum = torch.sum(logp, dim=1)\n",
    "    logp_cumsum = torch.cumsum(logp, dim=1)\n",
    "\n",
    "    # Get original model logits and compute KL penalties\n",
    "    with torch.no_grad(), model.disable_adapter():\n",
    "        outputs_orig = model(tokens, attention_mask=attention_mask)\n",
    "    logp_orig = dice.logp_categorical(outputs_orig.logits[:, input_len - 1 : -1], tokens[:, input_len:])\n",
    "    logp_orig_cumsum = torch.cumsum(logp_orig, dim=1)\n",
    "    kls = inv_cumsum(kl_div_est(logp_cumsum.detach(), logp_orig_cumsum.detach()))\n",
    "\n",
    "    # Compute rewards using evaluator model\n",
    "    texts = [tokenizer.decode(t, skip_special_tokens=True) for t in tokens]\n",
    "    prompts_all = make_evaluator_prompts(texts)\n",
    "    inputs_all = [\n",
    "        eval_tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "        for prompts in prompts_all\n",
    "    ]\n",
    "    with torch.no_grad():\n",
    "        outputs_all = [\n",
    "            eval_model(inputs.input_ids, attention_mask=inputs.attention_mask)\n",
    "            for inputs in inputs_all\n",
    "        ]\n",
    "    scores = torch.stack([get_scores(outputs.logits) for outputs in outputs_all], dim=1)\n",
    "    scores = soft_minimum(scores * signs_, weights_, tau=tau, dim=1)\n",
    "\n",
    "    # Create cost nodes and baselines, then backprop\n",
    "    losses_main = -F.logsigmoid(scores)\n",
    "    losses_main = dice.cost_node(losses_main, [logp_sum])\n",
    "    losses_main += baseline(losses_main, [logp_sum])\n",
    "    losses_kl = kls * kl_weight\n",
    "    losses_kl = dice.cost_node(losses_kl, [logp_cumsum])\n",
    "    losses_kl += baseline_kl(losses_kl, [logp_cumsum])\n",
    "    loss_main = losses_main.mean()\n",
    "    loss_kl = losses_kl.mean()\n",
    "    loss = loss_main + loss_kl\n",
    "    loss.backward()\n",
    "\n",
    "    # Print metrics\n",
    "    grad_norm = gradient_norm(model.parameters())\n",
    "    print(f\"step: {i}, loss: {loss.item():g}, main: {loss_main.item():g}, kl: {loss_kl.item():g}, grad norm: {grad_norm.item():g}\")\n",
    "\n",
    "    # Take an optimizer step\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AG2IUrohyt8-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading model...\n",
      "Initializing adapter...\n",
      "trainable params: 4,718,592 || all params: 167,041,536 || trainable%: 2.824801611019669\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#@title Load model to fine-tune\n",
    "\n",
    "model_name = \"EleutherAI/pythia-160m-deduped\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "peft_config = peft.LoraConfig(\n",
    "    peft.TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.0,\n",
    "    target_modules=[\n",
    "        \"attention.query_key_value\",\n",
    "        \"attention.dense\",\n",
    "        \"mlp.dense_h_to_4h\",\n",
    "        \"mlp.dense_4h_to_h\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Initializing adapter...\")\n",
    "model = peft.get_peft_model(model, peft_config)\n",
    "model.train()\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
